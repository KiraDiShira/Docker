# Architecture and theory

## Architecture Big Picture

All a container really is, is this ring fenced area of an operating system, with some limits on how much system resources it can use, and that's it. 

<img src="https://github.com/KiraDiShira/Docker/blob/master/ArchitectureAndTheory/Images/aat1.PNG" />

Now, to build them, we'll leverage a bunch of low-level kernel stuff, in particular we use namespaces and control groups.It's actually the Docker engine, that makes all of this really easy, and that's our two main topics. The kernel building blocks, and the Docker engine.

## Kernel Internals
Now, I know the container's started on Linux, and I know that Linux and Windows are very different beasts, but Docker's multi-platform these days, so I'm going to try and be as fair to Windows as I am to Linux. Well, here goes. Linux containers have been around for ages, or at least, the kernel stuff that we've build them with has, and I guess there's been a few people out there using them for ages, but for most of us, they were just too hard. You pretty much had to be a kernel hacker to get them working, and I suppose that's why it was really only the likes of Google and maybe a handful of other companies that were using them, the kind of companies, you know, that are only the bleeding edge with kernel engineers on staff, fair play, okay? They'd all seen the light while the rest of us were fumbling around in the dark and getting high on virtual machines. The point I want to make, though, is that the stuff to build containers is not new. It has been around in the Linux kernel for what seems like forever, definitely way before Docker was even a twinkle in Solomon's eye. Now, the Windows world was a bit different. Yeah, sure, Microsoft had a few internal projects, like Drawbridge and server silos, but those were dead in the water, and that's no disrespect to Microsoft, it's just a fact that the genesis of modern containers was all on Linux. Anyway, we used two main building blocks when we're building containers. Namespaces, and Control Groups. Both of them are Linux kernel primitives, and now, we've got equivalents on Windows. Hooray. Namespaces are about isolation, and Control Groups are about grouping objects and setting limits, so Namespaces first. These letters take an operating system, and carve it into multiple, isolated, virtual operating systems. It's a bit like hypervisors in virtual machines. So, in the hypervisor world, we take a single, physical machine, with all of its resources like CPU and RAM, and we carve out one or more virtual machines, and each one gets its own slice of virtual CPU, virtual memory, virtual networking, virtual storage, the whole shebang. Well, in the container world, we use Namespaces to take a single operating system with all of its resources, which tend to be high-level constructs like fire systems and process trees and users, and we carve all of that up into multiple virtual operating systems, called containers. Well, each container gets its own virtual or containerized root file system, it's own processed tree, it's own zero interface, it's own root user, the Full Monty. And just the way a virtual server in the hypervisor world looks, tastes, and smells like a regular, physical server in the container world, each container looks, smells, and feels exactly like a regular OS, only it's not. All three of these here are sharing a single kernel on the host, but everything's isolated, right? So that there's stuff inside of one container that doesn't even know about these others over here. It's ignorant! It's like, 'What? Other containers? What are you on about? There's only me here.' Well, in the Linux world, we know we've got Namespaces, and in the Windows world, we've also got Namespaces. Now, I've no doubt that the implementation specifics are different, but I suppose, to keep the jargon to a minimum, Windows folks are kindly referring to their OS isolation stuff as Namespaces as well, thank you for that. Now, in the Linux world, we've got these Namespaces, and Docker container is basically an organized collection of them. So, this container here, is it's own isolated grouping of these Namespaces. It's got its own process ID table with PID one and everything, its own network namespace with an id zero interface, IP address, its own root file system, blah blah blah. Oh, and it's secure, so it's a secure boundary, and yeah, obviously we can create more, each one isolated and looking and feeling like a standalone OS. Well, you know what, real quick: the PID Namespace like what I think we just hinted at, right, gives each container its own isolated process tree, complete with its very own PID one. This means, right, that a process in one container, say this one here, is blissfully unaware of any over here. Can't see them, doesn't even know they exist. Alright, well, then that Namespace gives each container its own isolated network stack, so its our next IP's rooting tables, the lot. Mount gives a container its own isolated root file system, that'd be C: on Windows, obviously / on Linux. IPC lets processes in a single container access the same shared memory, but it stops everything from outside of the container, isolation remember? UTS gives everything its own host name, and the username space which is actually relatively new to Docker, but that lets you map accounts inside the container to different users on the host. The typical example is mapping the container's root user to a non-privileged user on the host. Okay, well, the concepts the same for Linux and Windows, right? Slice and dice the OS, and provide isolation. So that's Namespaces, and it's great and all, but like, any multitanen system, there's always the fear of noisy neighbors. I mean, the last thing that this container here, oh, I've taken the Namespaces off, just to make it a bit easier on the eyes, right, the last thing Container A wants here is D over here throwing all night parties and chewing through all the CPU and RAM. So, right, to realistically have containers, you know, something that you'd run in production, we need something to polish the consumption of system resources. In the Linux world, this is control groups, and if you call, you called it C-groups. In Windows, it's Job Objects. But, credit to the Microsoft folks. They seem to be playing nice and generally calling them Control Groups as well. After all, right, they do a similar thing, and who wants multiple names for everything? But call them what you want, right? The idea is to group processes, and then impose limits. Now, I'm sure you've already got it, but Control Groups that are, say, okay, Container A over here is only going to get this amount of CPU, this amount of memory, and this amount of disk IO, and then, Container B, this amount, and these as well, and with these two technologies, Namespace and Control Groups, we have got a realistic shot at workable containers in a union file system, or some way of combining a bunch of read-only file systems or blocked devices, lashing away to the layer on top, and presenting to the system as a unified view. Take these three, and we have got modern containers, and that's exactly what Docker did. It came along and it made all of this easy, and the rest is history. Now, there is more, okay. There's always more. I mean, modern Docker containers leveraging things like capabilities and setcomp and a bunch of other stuff to add security and the likes, but honestly, these three are at the very center, and everything else is like icing on the cake, and that's enough, I think, of the kernel stuff. Time to switch tack and take a look at the Docker Engine.
